\chapter{Explainability}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: "thesis"
%%% End:
Understanding and interpreting labels in edge periodic temporal graphs is essential for various applications, such as network analysis, system monitoring, and predictive modeling.
However, assessing the ease with which humans can comprehend these labels presents a unique challenge.
In our earlier exploration of a simplified train schedule in Section \ref{ch:Introduction:sec:Motivation}, which applies to small scenarios with short factors that are easy to comprehend.
Handling real-world data with thousands of edges with labels with thousand time-steps, this simplicity of looking at a factor and guessing its semantic meaning gets lost.
In this context, we propose a comprehensive metric to measure the \enquote{explainability} of decompositions of such labels, taking into account multiple effects that influence their possibility to interpret.
This metric aims to enhance our ability to analyze a decomposition and comprehend the underlying factors, contributing to a more robust and insightful analysis.

\section{Explainability Metric}
The comprehensive nature of the proposed metric is crucial.
By considering factors like decomposition structure, precision, and size, it offers a holistic evaluation of label explainability.
A standardized metric facilitates comparative analysis across different graphs and so this comparison allows for the selection of methods and different parameters in the algorithm to make it more understandable by humans.
For a \DFA $A$, and its decomposition $(B_i)_{1 \leq i \leq k}$, we evaluate the following metrics:

\textbf{Decomposition Structure} ($\operatorname{DS} =  \underset{factor~ B_i}{\Pi}\frac{|B_i|}{\text{|A|}} \cdot (1 + \frac{out_{\{B_1,\dots,B_i\}}}{|F|})$)

The decomposition structure is the sum of all relative factor sizes, weighted by their relative amount of outliers.
It takes into account whether the label is presented as a single, continuous binary string or is split into multiple, shorter labels.
Labels with multiple shorter segments may facilitate explanation, as they enable a more granular understanding of connectivity changes at different time steps if they cover a relatively high amount of values.
For example, dividing the label into daily or weekly segments that represent more or less the long temporal label could aid in interpreting temporal patterns.
A few small factors with few outliers imply a small decomposition structure value, which is visible in Figure \ref{tab:metric-example-decompositions} for decomposition $D$, a large factor, covering not many values implies a high decomposition structure value, see decomposition $D''''$.
Generally, a low DS value is expected to be more explainable by a human interpreter. 

\textbf{Precision} ($\operatorname{precision} = \frac{|F| - out_{\{B_1,\dots,B_k\}}}{|F|}$)

Precision refers to the regularity of patterns in the label across different time steps.
A decomposition that has a high precision value, implies that there are few outliers and many of the actual values can be read from the factors.
Low precision implies that the decomposition covers only a few values for example in Figure \ref{tab:metric-example-decompositions} in decomposition $D'''$, requiring either a list of all outliers to fully understand the full picture.
A high precision is always preferred for an explainable label. 

\textbf{Size} ($k$)

The decomposition size is fundamental in assessing explainability.
A collection of labels with distinct information is generally easier for humans to understand, as it conveys information succinctly.
Therefore, generally, a decomposition with many factors is preferred over a decomposition with few factors.

To get some insight, consider the following imaginary example decompositions $\mathcal{D}$ to $\mathcal{D}''''$ of a \DFA with $|Q| = 100$ and $|F| = 50$.
A zero in Figure \ref{tab:metric-example-decompositions} for any given factor size, implies that the decomposition with largest factor $\mathcal{D}_i$ has zero outliers and covers the complete input \DFA with a factor of this size, a dash implies that a decomposition with largest factor of this size covers no final states at all.
\begin{table}[h]
	\centering
	\begin{tabular}{l|ccccc|lllcc}
		& \multicolumn{5}{c}{outliers of decomposition}  & & & & & \\
		& \multicolumn{5}{c}{with factor $|\mathcal{D}_i|$ of size} & DS & DS-c & DS-m & precision in \% & size \\
		& 4 & 5 & 10 & 20 & 50 & & & & & \\
		\hline
		$\mathcal{D}$ & 25 & 0 & &  & & 0.02 & 0.11 & 0.003 & 100 & 2\\		
		$\mathcal{D}'$ & - & 30 & 20 & 0 & & 0.07 & 0.22& 0.011 & 100 & 3\\
		$\mathcal{D}''$ & - & - & - & 40 & 2 & 0.18 & 0.88 & 0.187 &  96 & 2\\
		$\mathcal{D}'''$ & - & - & - & 45 & 29 & 0.47 & 1.13 & 0.3 & 26 & 2\\
		$\mathcal{D}''''$ & - & - & - & - & 40 & 0.40 & 0.90 & 0.9 & 20 & 1\\
	\end{tabular}
	\caption{Example decompositions and their outliers of combined factors}
	\label{tab:metric-example-decompositions}
\end{table}
In this example, $\mathcal{D}$ is considered to be a very good decomposition after all metrics.Â¸
It has only two factors of size 4 and 5, that cover the original \DFA with a precision of 100\%.
The decomposition structure is small as the two factors are small relative to the input size and the second factor is only included in the sum with a weight of 1 if there are no outliers.
$\mathcal{D}'$ is similarly well decomposed, with a small decomposition structure and also a precision of 100\%.
Although it has slightly larger factors, there might be a benefit in having more factors with different sizes when extracting semantic meaning.
$\mathcal{D}''$ only consists of only two larger factors, but only has a few hard outliers, compared to $\mathcal{D}'''$ with a precision of 26\%.
The last decomposition $\mathcal{D}''''$ has only a large factor with many hard outliers, which implies that it is hard to explain in a periodic way.
In general, a low decomposition structure as well as high precision and size, allow for greater \enquote{explainability}.

\section{Explainability Measurement}

To measure the proposed explainability metric for edge periodic temporal graphs, a systematic approach is employed, utilizing real-world data from face-to-face (F2F) graphs.
Existing algorithms for \DFA decomposition as well as novel approaches designed for explainable decomposition construction are used and the resulting decompositions are evaluated using the described metrics.
For a more complete analysis of the quality of the measured explainability, a study with a human-computer interaction focus would be required but is out of scope for this project.
Motivating the example from the introduction, an experiment quantifying the possibility with which humans can comprehend the structure of temporal or edge periodic graphs could include a train network. 
If two groups of people are given the same network, one group becomes a description in the form of a temporal graph and the other an edge periodic multi-graph.
Participants can then be asked questions regarding the traversal of the train network, or connectivity at certain times or intervals.
By measuring correctness and required time of the given answers, it should be possible to evaluate, which representation is easier to understand.  
Alternatively, a qualitative analysis would be possible by performing one-on-one interviews, where similar tasks are given but the explainability could be measured by letting participants describe, how complex understanding one or the other data structure feels.

