\chapter{Implementation}
\label{ch:Implementation}
%% ==============================
All algorithms described have been implemented using Kotlin, because it can be compiled for the Java Virtual Machine and for native execution, therefore it seemed like a good balance between a native implementation and higher language conciseness and fault tolerance. Also there were some libraries available for byte and bit operations on streams which proved to be quite useful, although there was almost no documentation available. This one and all other libraries are described in Section \ref{ch:Implementation:sec:Impl:subsec:libs}. The main focus is on the encoder and decoder classes but the other modules will be discussed as well. The project is realized as a maven project, to simplify dependency management. 

\section{Binary and Byte Wise RLE}
\label{ch:Implementation:bin and byte rle}
\par{
	The simple binary and byte wise RLE are rather trivial and implemented together in one encoder, the \textit{StringRunLengthEncoder}. The binary version is implemented with the mentioned BitStream from the IOStreams library. It allows working on a stream and reading and writing bit by bit and also reading the next $n$ bit as signed or unsigned number which comes in handy during decoding. In general, it is called with a variable $b$ bitsPerRun which sets the used bits to store one run. At first a maximum run length is determined by the maximum value $b$ can store as a binary string, $l(b) = n$ implies a maximum value of $2^{n-1}$. Then, the input is read consecutively in bits and the runs of equal bits are counted. We are always assuming the run starts with zero, if this is not the case a leading run of 0 is added, which means there are zero times 0 at the beginning. If a run exceeds the maximum run length, the maximum is written and again an artificial zero is added to the output stream to signal a length higher than the maximum. Each run can simply be written to the output stream with the desired amount of bits per run. During decoding, we assume the same $b$ and can then always read $n$ bits of the stream as unsigned value, know each run again and can therefore reconstruct the original data.
}
\par{
	Byte wise RLE is working with a similar idea of counting $n$ runs of equal information. This time it is applied on a byte level, reading byte after byte. If the next byte is the same as the current one, the counter is incremented, if not the run and the byte value are encoded as pair $(n, byte)$ to the output. The byte value itself still needs 8 bit of information but the run does not. Most raw untreated data does not contain long runs of consecutive identical byte values average run length is rather small. This implied storing a count of 1 or 2 in 8 bits of space which in turn explained the expansion in size seen in Table \ref{tab:t31 Byte-wise RLE on the Calgary Corpus}. Therefore, this version was also implemented with an arbitrary amount of bits to save per run, to minimize the overhead. If a run exceeds the maximum, which is again determined by the amount of bits stored per run, it is encoded twice, once with the maximum count and once with the remaining count. We do not need a zero run in this version because we also store the value itself, therefore we can count without a zero. This means for an example run of 4 times the value \textit{0xFF} and 4 bits per run saved, it will be encoded as the pair $(0011,0xFF)$ or $001111111111$ as consecutive binary stream.
}
\section{Vertical Binary RLE}
\label{ch:Implementation:vertical rle}
\par{
	Basically the ideas described in the Sections \ref{ch:Analysis:sec:Improvements by Preprocessing:subSec:vertReading} and \ref{ch:Conceptual Design:sec:Parallel Byte Reading} oppose only a small variance compared to regular binary RLE. It is realized with the use of BitStreams again. Its stream interface offers a position $p$, which corresponds to the byte value and an offset $o$, which is a bit value with significance $o$ of byte $p$, which allows reading all bits of the same significance in order. This was done for significance zero to seven to read all bits in a vertical manner as in the examples. Then, each run is again counted with the same method including a maximum run length defined by the amount of bits used to count a run and the runs are then written with the fixed amount of bits to the output stream. Afterwards, the amount of runs per bit position is written to the tail of the encoded file, without the information how many runs are expected it would be much more difficult to decide which run belongs where, but it is still possible. The average overhead of this additional information is around 34 byte, two for each count and a two byte stop symbol which is only needed seven times, no additional stop at the end of the file. Even though it was originally designed to work on chunks of bytes, in the end the transformations worked on the file or on the stream itself, which was significantly faster.
}
\par{
	During decoding, the expected amount of runs are parsed from the end of the file. Then the actual decoding happens, with the fixed $n$ bits per code for this bits significance. Knowing the exact amount of RLE numbers for each bit position makes it easy to decode, because the variable length of encoded numbers can be chosen accordingly while reading the stream once. Assuming a starting run on zero, all runs are written back to one file, each bit position sequentially, to then assemble the original data. This is done for each bit position in sequential order so we need to write 8 times to the output file. It might be worth trying out building the byte stream in memory and then write the output only once, which might be faster but also requires more internal data structures and holding the whole file in memory at once.
}
\section{Byte Remapping}
\label{ch:Implementation:bytemapping}
\par{
	To start of with the preprocessing, the byte remapping was implemented. The \emph{Analyzer} is responsible for generating an overall probability distribution over the values of bytes contained in the file. This serves as an input for the map generation, where every byte value is sorted accordingly to its occurrence and mapped to increasing byte values, so the most frequent byte to \textit{0x00}, the second most often to \textit{0x01} and so on. Afterwards, a temporary file is generated where each byte from the original file is mapped, which allows streaming during the encoding process. Decoding requires access to the original mapping, therefore it is persisted at the start of the encoded file. To do so, we only need to know the number of mapped values and then the original byte, not the mapped value since we know they are sorted acceding.
}
\section{Burrows Wheeler Transformation}
\label{ch:Implementation:bwt}
\par{
	As mentioned earlier, the Burrows-Wheeler-Transformation is implemented in 3 different versions, starting of with the naive vs.\ unsophisticated. The \emph{transformation.BurrowsWheelerTransformation} is implemented with the use of start and stop symbols (0x02 as STX and 0x03 as ETX) and with the creating and sorting of all cyclic rotations of the input string. This can be done for all text input files but not for binary data because files containing the start or stop symbols confused the algorithm and made the inverse transformation impossible. Additionally it is extremely slow due to its at least quadratic complexity, even when working on small chunks which messes up the overall transformation result. It is not further described as it was only used for some initial testing to see if and how much RLE benefits from this transformation. 
}
\par{
	The second implementation is realized by following the original algorithm description provided in greater detail in the paper by M. Burrows and D. J. Wheeler \cite{Burrows94} (algorithm C and D). The \emph{transformation.BurrowsWheelerTransformationModified} works on parts of the data so the transformation result is still strongly depending on the size of the chunks, but it could at least handle arbitrary input. Higher chunk sizes greatly increased the transformation because more equal characters are in the same chunk but also really slowed down the process. Due to the fact that both the mapping and the modified transformation work on an array of bytes and do not interfere with one-another, they can be performed in any order. The further on used advanced implementation of the bijective Burrows-Wheeler-Scott-Transformation is ported from Java from an external source or directly usable as dependency and therefore described in Section \ref{ch:Implementation:sec:Impl:subsec:libs}.
}
\section{Huffman Encoding}
\label{ch:Implementation:Huffman}
\par{
Following the pseudo-code provided by M. Li≈õkiewicz and H. Fernau in \cite{entropy-fernau} on page 21 the implementation was straight forward. Internally a small set of data structures are provided for assembling the Huffman tree, a \emph{HuffmanTree}, a \emph{HuffmanNode} and a \emph{HuffmanLeaf} class is implemented. The HuffmanTree is abstract and only holds the frequency of a tree, since every tree itself has its own frequency. It also implements a compareTo function, to draw comparisons between different trees. A HuffmanNode extends the HuffmanTree, consists of a left and right HuffmanTree and has their sum of frequencies as frequency. The leaf is itself also a tree and holds a value of type byte and a frequency which resembles its occurrence. To build the Huffman tree for a given set of bytes, the algorithm expects an array of integer $I$, assembling the occurrences $o$ of bytes $b \in [ 0,255 ]$ in the form of $I [ b ] = o$. Then a leaf is created for every entry of $I$ with a frequency of $o$ and collected into a PriorityQueue of HuffmanTrees. Then while there are still at least two trees left in the queue, the two lowest frequencies are removed from the queue, merged into a single tree and reinserted into the queue. After the creation of the tree, all paths are followed and every time a leaf is reached, the current path is added to a map in form of a StringBuffer $buf_b$. This buffer contains the path of left and right trees traversed into the original one, adding a zero for every left descent and a one for every right one. Finally the mapping of type byte to StringBuffer is returned. To apply this algorithm to the runs of the RLE encoding, the same is done except that instead byte values, every possible run length value is counted and collected into the same structure.
}
\par{
To reverse this Huffman coding it is required to have access to the performed mapping, otherwise decoding would be impossible. Therefore, the map itself is written to the beginning of the file, similar to the mapping from Section \ref{ch:Implementation:bytemapping}. This time though we need triplets of values, because the Huffman code can have a variable length, so the mapping is encoded to $(b,l(b),buf_b)$, with $b$ and $l(b)$ each assuming one byte. This also implies a maximum length for Huffman codes of 255. During decoding of the mapping, first the total number of mappings is parsed. Then, while there are still more mappings expected, we parse one byte which is the mapping value, then one byte which contains the length of the following mapping as unsigned byte value and then the Huffman code of the given length is parsed bit by bit and saved as a StringBuffer. This way it is possible to write and read the variable length codes continuously from the stream. 
}
\section{External Libraries}
\label{ch:Implementation:sec:Impl:subsec:libs}
\par{
Some external libraries are used throughout this project. Most of them provide a set of sealed functionality, like \emph{io.github.jupf.staticlog} which just facilitate the logging features and are not further interesting, therefore most of them are mentioned in Section \ref{ch:Implementation:sec:Impl:subsec:libs:others}, only extensively used ones are described in greater detail. 
}

\subsection{IOStreams for Kotlin}
\label{ch:Implementation:sec:Impl:subsec:libs:iostreams}
\par{
Alexander Kornilov created and released this library, which was found in the \href{https://discuss.kotlinlang.org/t/i-o-streams-for-kotlin/9802}{Kotlin forum}} and is currently hosted on \href{https://sourceforge.net/projects/kotlin-utils/}{Sourceforge}. It has a very light documentation \cite{IoStreamsKotlin} and is released with an Apache v2.0 license. It has to be mentioned that for the time being there is only a pre-release available, this version 0.33 is used throughout this project. Moving to this library rendered it possible to work entirely on streams of data as well as reading the next $n$ bits. This greatly improved the performance of the initial algorithm and reduced required data structures and memory. There was also some odd or unexpected behavior seen which might be changed in further versions, for example if the stream is currently at the first byte and we want to write to its bit with the highest significance. This represents a so called bit offset of 7, so we can write the position as 0:7. Writing a 1 or setting this bit to true advances the position of the stream to the next byte and offset zero, so 1:0 while writing a 0 keeps the stream at its current position so it is still at 0:7 and the next bit gets written on the same position. Basically the interface provides good functionality with the drawback of some inconsistencies.
}

\subsection{libDivSufSort}
\label{ch:Implementation:sec:Impl:subsec:libs:libDivSufSot}
\par{
The original code for the modified BWTS algorithm and the necessary sorting algorithms called \emph{DivSufSort} for efficient suffix array sorting was provided by Yuta Mori in the library \href{https://github.com/y-256/libdivsufsort}{libDivSufSort} \cite{LibDivSufSort} and is as already mentioned, closer described in the paper by Johannes Fischer and Florian Kurpicz \cite{DBLP:journals/corr/abs-1710-01896}. It  runs in $O(n \ log \ n)$ worst-case time using only $5n+O(1)$ bytes of memory space, where $n$ is the length of the input. The code is available under the MIT license at Github but written in C and thus, rather hard to use from Kotlin. Further research lead to a port to Java which uses the same structures and methods but is already close to the desired state, since Kotlin enables using Java classes by default.
}
\subsection{libDivSufSort in Java}
\label{ch:Implementation:sec:Impl:subsec:libs:libDivSufSort Java}
\par{
Porting the Java implementation provided by \href{https://github.com/flanglet/kanzi/releases}{Kanzi} \cite{kanzi}, a collection of state of the art compression methods, all available in Java, C++ and Go, was easy but also gratuitous, since there are releases available for the Java version which is still maintained at a high frequency. To use them we simply add the dependency to our maven project. This library basically provides one functionality, the implementation of a sophisticated bijective Burrows-Wheeler-Scott-Transformation in linear time.
}
\par{
To skim over its functionality, it provides a clean API to work with advanced compression algorithms in Java, most noticeable \emph{DivSufSort} and \emph{BWTS}. The BWTS class offers just the two directions of the transformation as methods, both working on arrays of bytes, which requires reading all the input into memory. This functionality is encapsulated by the class \emph{BWTSWrapper} to work on file level and generate a temporary file with the transformed contents to further work on a stream of that file. At first the whole input is spliced into Lyndon words. Then the suffix array is generated and sorted, using the fastest known suffix sorting algorithm DivSufSort. The result is also written to a temporary file, like the mapping which was quite useful for debugging, and enables streaming its contents during encoding. 
}

\subsection{Others}
\label{ch:Implementation:sec:Impl:subsec:libs:others}
\par{
Besides logging, some other basic functionality was added which was quite simple through the use of maven. \emph{org.junit.jupiter} provided the packages \emph{junit}, \emph{engine} and \emph{api} in version 5.6.0, which allows a simple test unit creation and execution. Additionally \emph{assert-j} in version 3.13.2 added extended assertion capabilities to the encoder as well as to the test cases. A modest approach of multithreading was enabled by use of Kotlin \emph{coroutines} in version 1.3.2, provided by Jetbrains. Just by convenience, Googles \emph{guava} was used in version 28.2 for new collection types and a comparator for an array of bytes.
}


%% ==============================
\section{Implementation Evaluation}
%% ==============================
\label{ch:Implementation:sec:Implementation Evaluation}
\par{
Obviously the assembled tool does not compete with the state of the art methods used today, neither in comparison of their compression results nor in terms of speed. There is most likely still a lot of unused potential to speed things up, for example by excessive multithreading. Also, decoding could be vastly speed up by writing each output byte only once instead of up to 8 times if the byte is of value 0xFF. Even compression results could probably be further improved but more on that in chapter \ref{ch:Discussion}. Nonetheless, the desired concept was proven and the results show a clear advantage over regular RLE achieved through preprocessing. 
}

%% ==============================
\section{Usage}
%% ==============================
\label{ch:Implementation:sec:usage}
\par{
To enable a convenient usage, the algorithm is obtainable as jar file but it can also be built from sources. It provides a simple command line interface which expects a desired action, either compressing \emph{-c} or decompressing \emph{-d} and a method, either vertical RLE \emph{-v}, binary \emph{-bin} or byte wise RLE \emph{-byte}. Binary and byte wise RLE can optionally be called with a parameter $N$ where $N$ is the amount of bits used to encode a single RLE number, vertical encoding can also run with an arbitrary amount of bits, but expects 8 comma separated numbers \emph{-v N,N,N,N,N,N,N,N}. As preprocessing options, mapping \emph{-map}, applying the sophisticated Burrows-Wheeler-Transformation \emph{-bwt}, and Huffman encoding \emph{-huf} are available via the parameters. Additional information can be acquired by launching the application with \emph{-h} for further help. To enable debug logging and get detailed insight into the compression and decompression steps, the parameter \emph{-D} has to be set. After all parameters have been set, a list of files is expected, started with the flag \emph{-f} and separated by space.
}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
