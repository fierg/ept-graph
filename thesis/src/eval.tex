%% eval.tex
%% $Id: eval.tex 61 2012-05-03 13:58:03Z bless $

\chapter{Evaluation}
\label{ch:Evaluation}
%% ==============================
Moving on to the evaluation, we start by comparing the achieved results using different preprocessing options or combination of those. To start of it is clear that the best overall compression ratio was accomplished by using a combination of different mapping techniques and a vertical interpretation of the input data. A compression ratio comparable to the state of the art was achieved and already suited files, like the file \emph{pic} from the Calgary Corpus, were compressed even further than with plain RLE. By taking a closer look at the discrepancy comparing each result, some clear differences between the preprocessing steps were shown. It is also clear that with the help of such methods, run length encoding can become a suitable compression algorithm for more than just pellet based images, although it is clearly not as sophisticated as advanced state of the art compression methods mentioned in Section \ref{ch:Principles of compression:sec:SOTA}.

%% ==============================
\section{Functional Evaluation}
%% ==============================
\label{ch:Evaluation:sec:Functional Evaluation}
\par{
The functional evaluation was performed in two steps. The first question is weather the algorithm works and no data is lost during the process of encoding. The decoder shows that this is the case and all information can be reconstructed, hence the suggested algorithm works as intended. The next question was, if it is just a method suited for this corpus and as it turned out, it performed even better on the newer and more frequently suggested Canterbury Corpus than it did on the Calgary Corpus.
}
	\begin{table}[h]
	\centering
	\begin{tabular}{r|r|r|r|r}	
		file & size original & size encoded & ratio in \% & \textit{bps}\\
		\hline
alice29.txt & 152089 & 65445 & 43.03 & 3.44 \\
asyoulik.txt & 125179 & 59291 & 47.36 & 3.79 \\
cp.html & 24603 & 11073 & 45.01 & 3.60 \\
fields.c & 11150 & 5183 & 46.48 & 3.72 \\
grammar.lsp & 3721 & 1923 & 51.68 & 4.13 \\
kennedy.xls & 1029744 & 229823 & 22.32 & 1.79 \\
lcet10.txt & 426754 & 170593 & 39.97 & 3.20 \\
plrabn12.txt & 481861 & 215628 & 44.75 & 3.58 \\
ptt5 & 513216 & 82136 & 16.01 & 1.28 \\
sum & 38240 & 19616 & 51.30 & 4.10 \\
xargs.1 & 4227 & 2515 & 59.50 & 4.76 \\
		\hline
		all files & 2814880 & 867322 & 30.81 & 2.46
	\end{tabular}
	\caption{Canterbury encoded, all preprocessing steps, using Huffman encoding for all counted runs.}
	\label{tab:t100:Canterbury encoded, all preprocessing steps, using Huffman encoding for all counted runs}
\end{table}
\par{
	The two desired goals could also be achieved, a significant compression for the whole corpus is possible while even improving against regular RLE on files highly suited for this method and the results are pretty similar on both corpora.
}

%% ==============================
\section{Benchmarks}
%% ==============================
\label{ch:Evaluation:sec:Benchmarks}
\par{
	To benchmark the proposed algorithms, it was compared against the state of the art on the same hardware. All benchmarks were performed on an AMD Ryzen 5 2600X six core processor (12 threads) with a 3.6 GHz base clock and a 4.2 GHz boost clock speed. For memory, 16GB 3200MHz ram and a Samsung evo ssd was used for persistent storage. The algorithm cmix explicitly demanded 32 GB of ram which could not be provided, which in turn lead to a lot of paging and thus, very poor timing benchmark results. Also, all algorithms were written in C and executed as binary executable, while the proposed algorithm \emph{modified vertical RLE} was written in Kotlin and executed on the GraalVM \cite{vsipek2019exploring}.
}
	\begin{table}[h]
	\begin{tabular}{r|r|r|r|r|r}
		method  &  size in bytes & compression & \textit{bps} & \multicolumn{2}{c}{time }\\
		& & & & encoding & decoding\\
		\hline
		uncompressed & 3,145,718 & 100.0\% & 8.00 & &\\
		compress 4.2.4 & 1,250,382 & 40.4\% & 3.24 & 0.039s & 0.025s\\
		modified vertical RLE & 1,237,380 & 39.3\%& 3.14 & 6.840s & 15.637s\\
		gzip v1.10 & 1,021,720 & 32.4\% & 2.60 & 0.232s & 0.025s\\
		ZIP v3.0 & 1,019,783 & 32.4\% & 2.59 & 0.214s & 0.022s\\
		zstandard 1.4.2& 887,004 & 28.1\% & 2.25 & 0.951s & 0.011s\\
		bzip2 v1.0.8 & 832,443 & 26.4\% & 2.11 & 0.191s & 0.088s\\
		brotli 1.0.7 & 826,638 & 26.3\%& 2.10 & 4.609s & 0.015s\\
		p7zip 16.02 (deflate) &  794,098 & 26.1\% & 2.08 & 0.431s & 0.045s \\
		p7zip 16.02 (PPMd) &  763,067& 24.2\% & 1.93 & 0.345s & 0.282s\\
		ZPAQ v7.15 & 659.700 & 20.9\% & 1.67 & 7.452s & 7.735s\\
		paq8hp* & - & - & - & - & -\\ 
		cmix v18 & 554,983 & 17.6\% & 1.41 & >3h & >2h		
	\end{tabular}
	\label{tab:t100benchmark}
	\caption{Benchmark on the Calgary Corpus.}
\end{table}
\par{
	Avoiding internal operations and large or complex data structures to hold all the input data or even collecting the values of same significance in memory into byte arrays greatly improved time performance of the algorithm described. Encoding is reasonable fast with measured 6.8 seconds but the decoding is rather slow with 15.6 seconds although it has to be mentioned that there is still some potential in performance optimization and parallelization. The main reason for this margin between encoding and decoding speed is most likely due to the multiple writing of the output file, since each bit position has to be decoded separately so in the worst case a single byte need to be written 8 times if it contains $0xFF$. 
}

%% ==============================
\section{Conclusion}
%% ==============================
\label{ch:Evaluation:sec:Conclusion}
\par{
In conclusion, the desired state was achieved and it was shown that with the help of preprocessing and a different encoding technique, RLE can achieve compression results comparable to modern methods. It is still far from opposing a real competition, neither in speed nor compression ratios, but it is only a few percent points behind daily used algorithms. Possible additional improvements will be discussed in the next section, although at this point one has to decide if compression or speed should be in the main focus. Anyhow, to show the efficiency of the suggested preprocessing methods, it is definitely adequate.
}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
